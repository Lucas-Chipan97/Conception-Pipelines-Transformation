# Projet ETL - Nettoyage de Données

> Pipeline complet de nettoyage et transformation de données pour l'analyse ETL

## Description

Ce projet propose une suite complète d'outils Python pour le nettoyage et la transformation de données dans un contexte ETL. Il permet d'analyser, nettoyer et valider automatiquement la qualité des données avec gestion robuste des erreurs.

## Fonctionnalités

- **Analyse automatique des données** - Détection des problèmes de qualité
- **Traitement des valeurs manquantes** - Stratégies adaptées par type de colonne
- **Gestion des valeurs aberrantes** - Détection IQR et ajustements intelligents
- **Suppression des doublons** - Nettoyage complet et IDs uniques
- **Validation croisée** - Contrôles de qualité après traitement
- **Gestion d'erreurs robuste** - Try/except et logging complet
- **Sauvegarde automatique** - Fichiers nettoyés avec validation

## Installation

```bash
# Cloner le repository
git clone https://github.com/votre-username/etl-data-cleaning.git
cd etl-data-cleaning

# Installer les dépendances
pip install pandas numpy
```

## Utilisation rapide

```python
# Script complet - tout en un
python nettoyage_donnees_complet.py

# Scripts individuels
python analyse_donnees.py
python traitement_valeurs_manquantes.py
python gestion_valeurs_aberrantes.py
```

## Scripts disponibles

### 1. analyse_donnees.py
Analyse initiale complète du jeu de données
- Détection des valeurs manquantes
- Identification des doublons
- Recherche des valeurs aberrantes
- Statistiques descriptives

### 2. traitement_valeurs_manquantes.py
Gestion intelligente des valeurs manquantes
- Remplacement par médiane/moyenne
- Valeurs par défaut contextuelles
- Stratégies par type de colonne

### 3. gestion_valeurs_aberrantes.py
Traitement des valeurs aberrantes
- Méthode IQR pour la détection
- Plafonnement des valeurs extrêmes
- Suppression des valeurs impossibles

### 4. nettoyage_donnees_complet.py
Pipeline complet de nettoyage
- Traitement automatique de bout en bout
- Validation croisée intégrée
- Gestion d'erreurs complète

## Exemples

### Analyse simple
```python
import pandas as pd

# Charger et analyser
df = pd.read_csv('donnees.csv')
print(f"Lignes: {len(df)}")
print(f"Valeurs manquantes: {df.isnull().sum().sum()}")
```

### Nettoyage complet
```python
from nettoyage_donnees_complet import nettoyer_donnees

# Nettoyer automatiquement
donnees_propres = nettoyer_donnees('donnees.csv')
print(f"Données nettoyées sauvées dans: donnees_nettoye.csv")
```

## Structure du projet

```
etl-data-cleaning/
├── analyse_donnees.py
├── traitement_valeurs_manquantes.py
├── gestion_valeurs_aberrantes.py
├── nettoyage_donnees_complet.py
├── donnees_test/
│   └── jeu_donnees_etl_5000_lignes.csv
├── resultats/
│   ├── donnees_nettoye.csv
│   └── rapport_qualite.txt
└── README.md
```

## Configuration

### Paramètres par défaut
- **Seuil IQR**: 1.5 (détection aberrants)
- **Valeurs manquantes**: Médiane pour quantités, moyenne pour prix
- **Doublons**: Conservation du dernier enregistrement
- **Logging**: Level INFO avec horodatage

### Personnalisation
Modifiez les variables en début de script :
```python
SEUIL_IQR = 1.5
STRATEGIE_DOUBLONS = 'last'  # 'first' ou 'last'
VALEUR_DEFAUT_PRODUIT = 'Produit_Inconnu'
```

## Stratégies de nettoyage

### Valeurs manquantes
- **ID_produit**: Suppression des lignes (obligatoire)
- **Nom_produit**: Remplacement par "Produit_Inconnu"  
- **Quantite_vendue**: Imputation par la médiane
- **Prix_unitaire**: Imputation par la moyenne
- **Date_vente**: Valeur par défaut "1900-01-01"

### Valeurs aberrantes
- **Quantités négatives**: Suppression complète
- **Prix négatifs/zéros**: Suppression complète
- **Valeurs extrêmes**: Plafonnement aux limites IQR
- **Ajustements**: Remplacement par médiane si nécessaire

### Doublons
- **Lignes identiques**: Suppression complète
- **IDs dupliqués**: Conservation du dernier enregistrement
- **Validation**: Vérification de l'unicité des IDs

## Validation de qualité

Le système effectue automatiquement ces contrôles :
- ✅ Absence de valeurs manquantes
- ✅ Unicité des identifiants
- ✅ Cohérence des plages de valeurs
- ✅ Intégrité des types de données
- ✅ Validation des règles métier

## Gestion d'erreurs

Erreurs gérées automatiquement :
- Fichiers introuvables
- Formats CSV corrompus
- Colonnes manquantes
- Erreurs de types de données
- Problèmes de sauvegarde

## Logs et monitoring

Les logs incluent :
- Horodatage de chaque opération
- Nombre d'éléments traités
- Erreurs rencontrées
- Statistiques de transformation
- Validation des résultats

## Performance

### Temps d'exécution typiques
- 5K lignes: ~2 secondes
- 50K lignes: ~15 secondes  
- 500K lignes: ~2 minutes

### Optimisations
- Traitement vectorisé avec Pandas
- Validation par chunks pour gros volumes
- Logging optimisé pour la performance

## Contribution

1. Fork du projet
2. Créer une branche feature
3. Commit des modifications
4. Push vers la branche
5. Ouvrir une Pull Request

### Standards de code
- PEP 8 pour le style Python
- Docstrings pour toutes les fonctions
- Tests unitaires recommandés
- Gestion d'erreurs obligatoire

## Versions

### v1.0.0 - Version initiale
- Scripts de base pour nettoyage
- Gestion des valeurs manquantes
- Traitement des aberrants

### v1.1.0 - Améliorations
- Pipeline complet intégré
- Validation croisée
- Gestion d'erreurs robuste

### v1.2.0 - Fonctionnalités avancées
- Logging détaillé
- Optimisations performance
- Documentation complète
