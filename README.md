# Pipeline ETL Complet - Extract, Transform & Load

> Ce projet Python propose un pipeline ETL complet incluant la transformation des donnÃ©es et leur chargement avec historisation. Il implÃ©mente toutes les Ã©tapes du processus ETL : nettoyage, transformation, validation, insertion SQL, UPSERT, et sauvegarde historique avec timestamps.

## ğŸ›  FonctionnalitÃ©s principales

### **Phase Transform** ğŸ“Š
- **Analyse complÃ¨te** : dimensions, valeurs manquantes, doublons, valeurs aberrantes
- **Nettoyage robuste** : traitement des valeurs manquantes, suppression d'anomalies, dÃ©doublonnage
- **Transformations avancÃ©es** : colonnes calculÃ©es, catÃ©gories, normalisation Min-Max
- **AgrÃ©gation intelligente** : rÃ©sumÃ©s statistiques par produit avec mÃ©triques de performance
- **Validation croisÃ©e** : contrÃ´les qualitÃ© automatiques et cohÃ©rence des donnÃ©es
- **Tests unitaires** : validation sur donnÃ©es synthÃ©tiques avant traitement rÃ©el

### **Phase Load** ğŸ’¾
- **Insertion SQL** : INSERT simple, UPSERT, TRUNCATE INSERT
- **Base de donnÃ©es** : SQLite intÃ©grÃ©e avec crÃ©ation automatique des tables
- **Historisation** : sauvegarde automatique avec timestamps pour traÃ§abilitÃ©
- **Backup prÃ©ventif** : sauvegarde de la base avant modifications
- **Rapports de chargement** : statistiques dÃ©taillÃ©es post-insertion

## ğŸ“¦ DÃ©pendances

```bash
pip install pandas numpy sqlalchemy
```

**Librairies utilisÃ©es :**
- `pandas` - manipulation des DataFrames
- `numpy` - calculs numÃ©riques et normalisation
- `sqlalchemy` - ORM pour les opÃ©rations SQL
- `sqlite3` - base de donnÃ©es embarquÃ©e
- `logging`, `datetime`, `os`, `shutil` (bibliothÃ¨ques standard)

## ğŸš€ Utilisation

### **Mode Transform uniquement**
```python
from etl_complet_simplifie import pipeline_etl_complet

# Pipeline de transformation
donnees_finales, agregats = pipeline_etl_complet("donnees_ventes.csv")
```

### **Mode ETL Complet (Transform + Load)**
```python
from pipeline_etl_load_complet import PipelineETLAvecLoad

# Pipeline complet avec base de donnÃ©es
pipeline = PipelineETLAvecLoad()
pipeline.pipeline_complet("donnees_ventes.csv", mode_chargement="upsert")
```

### **Modes de chargement disponibles**
- `"insert"` - Insertion simple des nouvelles donnÃ©es
- `"upsert"` - Mise Ã  jour si existe, insertion sinon (recommandÃ©)
- `"truncate_insert"` - Vider la table puis insÃ©rer (donnÃ©es fraÃ®ches)

## ğŸ“Š Architecture du Pipeline

### **ğŸ”„ Phase EXTRACT**
```
Fichier CSV â†’ DataFrame Pandas â†’ Validation initiale
```

### **ğŸ§¹ Phase TRANSFORM**
1. **Analyse des donnÃ©es**
   - Dimensions et types
   - Valeurs manquantes par colonne
   - DÃ©tection de doublons et aberrants

2. **Nettoyage automatisÃ©**
   - `Nom_produit` â†’ "Produit_Inconnu" si manquant
   - `Quantite_vendue` â†’ mÃ©diane si manquant
   - `Prix_unitaire` â†’ moyenne si manquant
   - Suppression des valeurs aberrantes (IQR)
   - DÃ©duplication intelligente

3. **Transformations enrichies**
   - `Chiffre_affaires` = QuantitÃ© Ã— Prix
   - `Categorie_prix` : Bas/Moyen/Ã‰levÃ©/Premium
   - `Categorie_volume` : Faible/Moyen/Ã‰levÃ©/TrÃ¨s_Ã©levÃ©
   - Normalisation Min-Max : valeurs entre 0 et 1
   - `Timestamp_traitement` automatique

4. **AgrÃ©gation par produit**
   - Somme, moyenne, count des quantitÃ©s
   - Min, max, moyenne des prix
   - Total et moyenne du chiffre d'affaires
   - Score de performance automatique

5. **Validation croisÃ©e**
   - CohÃ©rence des calculs
   - IntÃ©gritÃ© des transformations
   - Tests de qualitÃ© (8 critÃ¨res)

### **ğŸ’¾ Phase LOAD**
1. **PrÃ©paration base de donnÃ©es**
   - CrÃ©ation automatique des tables (`ventes`, `agregats_produits`)
   - Sauvegarde prÃ©ventive
   - Gestion des contraintes

2. **Chargement intelligent**
   - **INSERT** : Ajout simple
   - **UPSERT** : `INSERT ... ON DUPLICATE KEY UPDATE`
   - **TRUNCATE INSERT** : Remplacement complet

3. **Historisation complÃ¨te**
   - Sauvegarde horodatÃ©e de chaque Ã©tape
   - Organisation en rÃ©pertoires (`historique_donnees/`, `backup_donnees/`)
   - TraÃ§abilitÃ© complÃ¨te des traitements

## ğŸ“ Structure de fichiers gÃ©nÃ©rÃ©e

```
projet_etl/
â”œâ”€â”€ etl_complet_simplifie.py          # Pipeline Transform uniquement
â”œâ”€â”€ pipeline_etl_load_complet.py      # Pipeline ETL complet
â”œâ”€â”€ data_warehouse.db                 # Base de donnÃ©es SQLite
â”œâ”€â”€ historique_donnees/               # Historique horodatÃ©
â”‚   â”œâ”€â”€ donnees_brutes_20241222_143022.csv
â”‚   â”œâ”€â”€ donnees_nettoyees_20241222_143022.csv
â”‚   â”œâ”€â”€ agregats_20241222_143022.csv
â”‚   â””â”€â”€ rapport_load_20241222_143022.csv
â”œâ”€â”€ backup_donnees/                   # Sauvegardes DB
â”‚   â””â”€â”€ backup_db_20241222_143022.db
â”œâ”€â”€ jeu_donnees_etl_5000_lignes_nettoye.csv    # RÃ©sultats Transform
â”œâ”€â”€ jeu_donnees_etl_5000_lignes_agregats.csv   # AgrÃ©gats
â””â”€â”€ README.md
```

## ğŸ¯ Exemples d'utilisation

### **Test rapide avec donnÃ©es synthÃ©tiques**
```python
# Mode interactif avec donnÃ©es de test
python etl_complet_simplifie.py
# Choisir 'o' pour tester avec donnÃ©es synthÃ©tiques

python pipeline_etl_load_complet.py
# Choisir le mode de chargement souhaitÃ©
```

### **Traitement de donnÃ©es rÃ©elles**
```python
# Transform seul
from etl_complet_simplifie import pipeline_etl_complet
donnees, agregats = pipeline_etl_complet("mes_donnees.csv")

# ETL complet avec Load
from pipeline_etl_load_complet import PipelineETLAvecLoad
pipeline = PipelineETLAvecLoad()
pipeline.pipeline_complet("mes_donnees.csv", "upsert")
```

## ğŸ“‹ Validation et contrÃ´les qualitÃ©

### **Tests automatiques (Transform)**
- âœ… Valeurs manquantes traitÃ©es
- âœ… Doublons supprimÃ©s  
- âœ… Valeurs aberrantes Ã©liminÃ©es
- âœ… Colonnes calculÃ©es ajoutÃ©es
- âœ… Normalisation appliquÃ©e
- âœ… AgrÃ©gats crÃ©Ã©s
- âœ… CohÃ©rence des calculs
- âœ… Validation croisÃ©e

### **ContrÃ´les de chargement (Load)**
- ğŸ—„ï¸ Tables crÃ©Ã©es automatiquement
- ğŸ’¾ Sauvegarde prÃ©ventive effectuÃ©e
- ğŸ”„ Mode UPSERT/INSERT exÃ©cutÃ©
- ğŸ“Š Rapport de chargement gÃ©nÃ©rÃ©
- ğŸ—‚ï¸ Historisation complÃ¨te
- âœ… IntÃ©gritÃ© des donnÃ©es vÃ©rifiÃ©e

## ğŸ§ª Gestion d'erreurs et logging

```python
# Logging automatique
2024-12-22 14:30:22 - INFO - DonnÃ©es chargÃ©es: jeu_donnees_etl_5000_lignes.csv
2024-12-22 14:30:23 - INFO - Historisation: donnees_brutes_20241222_143022.csv
2024-12-22 14:30:24 - INFO - Base de donnÃ©es sauvegardÃ©e: backup_db_20241222_143022.db
```

**Gestion robuste :**
- Try/catch Ã  tous les niveaux
- Logging dÃ©taillÃ© des opÃ©rations
- Validation Ã©tape par Ã©tape
- Sauvegarde prÃ©ventive
- Recovery automatique

## ğŸ“Š DonnÃ©es d'entrÃ©e attendues

**Format CSV avec colonnes :**
- `ID_produit` (int) - Identifiant unique
- `Nom_produit` (str) - Nom du produit
- `Quantite_vendue` (float) - QuantitÃ© vendue
- `Prix_unitaire` (float) - Prix unitaire
- `Date_vente` (str) - Date de vente (YYYY-MM-DD)

**Le pipeline gÃ¨re automatiquement :**
- Valeurs manquantes dans toutes les colonnes
- Doublons sur ID_produit ou lignes complÃ¨tes
- Valeurs aberrantes (nÃ©gatives, zÃ©ros, extrÃªmes)
- Formats de dates diverses
- IncohÃ©rences de donnÃ©es

## ğŸ”§ Configuration avancÃ©e

```python
# Personnalisation du pipeline Load
pipeline = PipelineETLAvecLoad(db_path="ma_base.db")
pipeline.historique_dir = "mon_historique"
pipeline.backup_dir = "mes_sauvegardes"
```

## âœ… RÃ©sultats garantis

**Transform :**
- âœ¨ DonnÃ©es 100% nettoyÃ©es et validÃ©es
- ğŸ“ˆ Enrichissement avec colonnes calculÃ©es
- ğŸ¯ Normalisation et catÃ©gorisation
- ğŸ“Š AgrÃ©gats statistiques par produit
- ğŸ§ª Tests de qualitÃ© automatiques

**Load :**
- ğŸ—„ï¸ Base de donnÃ©es SQLite structurÃ©e
- ğŸ“š Historique complet avec timestamps  
- ğŸ’¾ Sauvegardes automatiques
- ğŸ“‹ Rapports de chargement dÃ©taillÃ©s
- ğŸ”„ Modes de chargement flexibles

---

## ğŸš€ Quick Start

```bash
# 1. Installer les dÃ©pendances
pip install pandas numpy sqlalchemy

# 2. Test rapide Transform
python etl_complet_simplifie.py

# 3. Test complet Transform + Load  
python pipeline_etl_load_complet.py

# 4. VÃ©rifier les rÃ©sultats
ls -la *.csv historique_donnees/ backup_donnees/
```

**ğŸ‰ Votre pipeline ETL est prÃªt !**

---

Â© Projet pÃ©dagogique ETL - Extract, Transform & Load avec Python â€¢ DonnÃ©es fictives â€¢ Pipeline production-ready
