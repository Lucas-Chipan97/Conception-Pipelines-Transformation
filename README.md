# Projet ETL - Transformation et QualitÃ© des DonnÃ©es

> Pipeline complet pour le traitement, l'enrichissement et la transformation robuste de donnÃ©es dans un contexte ETL.

## Objectif

Assurer un flux de traitement des donnÃ©es fiable, automatisÃ© et Ã©volutif, incluant toutes les Ã©tapes clÃ©s du cycle de vie dâ€™un jeu de donnÃ©es en ETL : nettoyage, validation, transformation, enrichissement, normalisation, avec gestion dâ€™erreurs intÃ©grÃ©e.

## Ã‰tapes du Pipeline

### ğŸ”¹ 1. Nettoyage des donnÃ©es
- Suppression des doublons (via `np.unique` ou `pandas.drop_duplicates`)
- Remplacement des valeurs manquantes (`np.nanmean`, `fillna`)
- Fusion des entitÃ©s similaires (logique mÃ©tier ou fuzzy matching)
- Normalisation initiale (min-max ou z-score)

### ğŸ”¹ 2. Validation des donnÃ©es
- ConformitÃ© aux types et formats (dates, numÃ©riques, etc.)
- ContrÃ´le de plages (valeurs rÃ©alistes uniquement)
- VÃ©rification des clÃ©s primaires/Ã©trangÃ¨res
- CohÃ©rence intercolonnes (ex. : Ã¢ge â†” date de naissance)

### ğŸ”¹ 3. Transformation des donnÃ©es
- Calculs dÃ©rivÃ©s (TVA, revenus, dÃ©lais, etc.)
- AgrÃ©gation (groupes par rÃ©gion, produit, etc.)
- Conversion de formats (dates, devises, unitÃ©s)
- Ajout de colonnes mÃ©tier ou horodatÃ©es
- Adaptation aux besoins spÃ©cifiques de lâ€™entreprise

### ğŸ”¹ 4. Enrichissement
- IntÃ©gration de donnÃ©es externes (dÃ©mographie, segments)
- Ajout de colonnes issues de mappings ou de regroupements
- Personnalisation client / produit
- PrÃ©paration pour la segmentation, analyse ou visualisation

### ğŸ”¹ 5. Normalisation
- Mise Ã  lâ€™Ã©chelle des variables (min-max / z-score)
- RÃ©duction de redondance pour assurer la qualitÃ©
- Structuration des donnÃ©es pour lâ€™entreposage
- ConformitÃ© aux bonnes pratiques de modÃ©lisation

### ğŸ”¹ 6. Gestion des erreurs
- `try/except` avec messages explicites
- Journalisation automatique avec dÃ©tails des erreurs
- Mise en quarantaine des lignes corrompues
- Reprise du traitement sans perte de donnÃ©es

## Exemple de pipeline (simplifiÃ©)

```python
from pipeline_etl import executer_pipeline

executer_pipeline("ventes_brutes.csv")
# RÃ©sultat: ventes_brutes_nettoyees.csv, logs.txt, rapport_qualite.txt
```

## Bonnes pratiques appliquÃ©es

- **Logging horodatÃ©** et suivi des erreurs
- **Validation croisÃ©e** Ã  chaque Ã©tape
- **Variables paramÃ©trables** pour lâ€™adaptabilitÃ©
- **Tests unitaires recommandÃ©s** avant intÃ©gration
- **Documentation incluse** dans les scripts

## Configurations possibles

```python
SEUIL_IQR = 1.5
FORMAT_DATE_ATTENDU = "%Y-%m-%d"
TAUX_TVA = 20
STRATEGIE_VALEURS_MANQUANTES = {
    "Prix": "moyenne",
    "QuantitÃ©": "mÃ©diane",
    "Date": "default"
}
```

## RÃ©sultats attendus

- âœ… DonnÃ©es sans doublons ni valeurs aberrantes
- âœ… Variables harmonisÃ©es et cohÃ©rentes
- âœ… AgrÃ©gats prÃªts pour reporting ou BI
- âœ… Logs clairs et auditables
- âœ… TraÃ§abilitÃ© complÃ¨te de chaque transformation

## Version actuelle

**v1.3.0 - Pipeline robuste avec enrichissement et normalisation**

- Enrichissement externe intÃ©grÃ©
- CrÃ©ation dynamique de colonnes
- Gestion des exceptions amÃ©liorÃ©e
- Journalisation multi-niveaux
